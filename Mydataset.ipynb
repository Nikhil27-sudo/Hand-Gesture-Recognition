{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# organize imports\n",
    "import cv2\n",
    "import imutils\n",
    "import numpy as np\n",
    "\n",
    "# global variables\n",
    "bg = None\n",
    "\n",
    "def run_avg(image, aWeight):\n",
    "    global bg\n",
    "    # initialize the background\n",
    "    if bg is None:\n",
    "        bg = image.copy().astype(\"float\")\n",
    "        return\n",
    "\n",
    "    # compute weighted average, accumulate it and update the background\n",
    "    cv2.accumulateWeighted(image, bg, aWeight)\n",
    "\n",
    "def segment(image, threshold=25):\n",
    "    global bg\n",
    "    # find the absolute difference between background and current frame\n",
    "    diff = cv2.absdiff(bg.astype(\"uint8\"), image)\n",
    "\n",
    "    # threshold the diff image so that we get the foreground\n",
    "    thresholded = cv2.threshold(diff,\n",
    "                                threshold,\n",
    "                                255,\n",
    "                                cv2.THRESH_BINARY)[1]\n",
    "\n",
    "    # get the contours in the thresholded image\n",
    "    (cnts, _) = cv2.findContours(thresholded.copy(),\n",
    "                                    cv2.RETR_EXTERNAL,\n",
    "                                    cv2.CHAIN_APPROX_SIMPLE)\n",
    "\n",
    "    # return None, if no contours detected\n",
    "    if len(cnts) == 0:\n",
    "        return\n",
    "    else:\n",
    "        # based on contour area, get the maximum contour which is the hand\n",
    "        segmented = max(cnts, key=cv2.contourArea)\n",
    "        return (thresholded, segmented)\n",
    "\n",
    "def create_folder(folder_name):\n",
    "\tif not os.path.exists(folder_name):\n",
    "\t\tos.mkdir(folder_name)\n",
    "\n",
    "def main():\n",
    "    # initialize weight for running average\n",
    "    aWeight = 0.5\n",
    "\n",
    "    # get the reference to the webcam\n",
    "    camera = cv2.VideoCapture(0)\n",
    "\n",
    "    # region of interest (ROI) coordinates\n",
    "    top, right, bottom, left = 70, 350, 300, 590\n",
    "\n",
    "    # initialize num of frames\n",
    "    num_frames = 0\n",
    "    image_num = 1\n",
    "\n",
    "    start_recording = False\n",
    "\n",
    "    # keep looping, until interrupted\n",
    "    while(True):\n",
    "        # get the current frame\n",
    "        (grabbed, frame) = camera.read()\n",
    "        if (grabbed == True):\n",
    "\n",
    "            # resize the frame\n",
    "            frame = imutils.resize(frame, width=700)\n",
    "\n",
    "            # flip the frame so that it is not the mirror view\n",
    "            frame = cv2.flip(frame, 1)\n",
    "\n",
    "            # clone the frame\n",
    "            clone = frame.copy()\n",
    "\n",
    "            # get the height and width of the frame\n",
    "            (height, width) = frame.shape[:2]\n",
    "\n",
    "            # get the ROI\n",
    "            roi = frame[top:bottom, right:left]\n",
    "\n",
    "            # convert the roi to grayscale and blur it\n",
    "            gray = cv2.cvtColor(roi, cv2.COLOR_BGR2GRAY)\n",
    "            gray = cv2.GaussianBlur(gray, (7, 7), 0)\n",
    "\n",
    "            # to get the background, keep looking till a threshold is reached\n",
    "            # so that our running average model gets calibrated\n",
    "            if num_frames < 30:\n",
    "                run_avg(gray, aWeight)\n",
    "#                 print(num_frames)\n",
    "            else:\n",
    "                # segment the hand region\n",
    "                hand = segment(gray)\n",
    "\n",
    "                # check whether hand region is segmented\n",
    "                if hand is not None:\n",
    "                    # if yes, unpack the thresholded image and\n",
    "                    # segmented region\n",
    "                    (thresholded, segmented) = hand\n",
    "\n",
    "                    # draw the segmented region and display the frame\n",
    "                    cv2.drawContours(clone, [segmented + (right, top)], -1, (0, 0, 255))\n",
    "                    if start_recording:\n",
    "\n",
    "                        # Mention the directory in which you wanna store the images followed by the image name\n",
    "                        cv2.imwrite(\"C:/Users/NIKHIL/Desktop/Mydataset/31/\" + str(image_num) + '.png', thresholded)\n",
    "                        image_num += 1\n",
    "                    cv2.imshow(\"Thesholded\", thresholded)\n",
    "\n",
    "            # draw the segmented hand\n",
    "            cv2.rectangle(clone, (left, top), (right, bottom), (0,255,0), 2)\n",
    "\n",
    "            # increment the number of frames\n",
    "            num_frames += 1\n",
    "            # print(num_frames)\n",
    "\n",
    "            # display the frame with segmented hand\n",
    "            cv2.imshow(\"Video Feed\", clone)\n",
    "\n",
    "            # observe the keypress by the user\n",
    "            keypress = cv2.waitKey(1) & 0xFF\n",
    "\n",
    "            # if the user pressed \"q\", then stop looping\n",
    "            if keypress == ord(\"q\") or image_num > 1:\n",
    "                break\n",
    "        \n",
    "            if keypress == ord(\"s\"):\n",
    "                start_recording = True\n",
    "\n",
    "        else:\n",
    "            print(\"[Warning!] Error input, Please check your(camra Or video)\")\n",
    "            break\n",
    "    # free up memory\n",
    "    camera.release()\n",
    "    cv2.destroyAllWindows()\n",
    "main()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "from glob import glob\n",
    "import os\n",
    "def resizeImage(imageName):\n",
    "    basewidth = 100\n",
    "    img = Image.open(imageName)\n",
    "    wpercent = (basewidth/float(img.size[0]))\n",
    "    hsize = int((float(img.size[1])*float(wpercent)))\n",
    "    img = img.resize((100, 89), Image.ANTIALIAS)\n",
    "    img.save(imageName)\n",
    "\n",
    "for i in range(1, 10):\n",
    "    os.chdir(\"C:/Users/NIKHIL/Desktop/Mydataset/\")\n",
    "    for file in glob(\"*/*.png\"):\n",
    "        resizeImage(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 95  | total loss: \u001b[1m\u001b[32m2.38091\u001b[0m\u001b[0m | time: 1.931s\n",
      "\u001b[2K\r",
      "| Adam | epoch: 002 | loss: 2.38091 - acc: 0.2278 -- iter: 0320/5728\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-3ca63138b6d7>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     85\u001b[0m model.fit(loadedImages, outputVectors, n_epoch=50,\n\u001b[0;32m     86\u001b[0m            \u001b[0mvalidation_set\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mtestImages\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtestLabels\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 87\u001b[1;33m            snapshot_step=100, show_metric=True, run_id='convnet_coursera')\n\u001b[0m\u001b[0;32m     88\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     89\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"TrainedModel/GestureRecogModel.tfl\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tflearn\\models\\dnn.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X_inputs, Y_targets, n_epoch, validation_set, show_metric, batch_size, shuffle, snapshot_epoch, snapshot_step, excl_trainops, validation_batch_size, run_id, callbacks)\u001b[0m\n\u001b[0;32m    214\u001b[0m                          \u001b[0mexcl_trainops\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mexcl_trainops\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    215\u001b[0m                          \u001b[0mrun_id\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mrun_id\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 216\u001b[1;33m                          callbacks=callbacks)\n\u001b[0m\u001b[0;32m    217\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    218\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mfit_batch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_inputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY_targets\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tflearn\\helpers\\trainer.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, feed_dicts, n_epoch, val_feed_dicts, show_metric, snapshot_step, snapshot_epoch, shuffle_all, dprep_dict, daug_dict, excl_trainops, run_id, callbacks)\u001b[0m\n\u001b[0;32m    337\u001b[0m                                                        \u001b[1;33m(\u001b[0m\u001b[0mbool\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbest_checkpoint_path\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m|\u001b[0m \u001b[0msnapshot_epoch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    338\u001b[0m                                                        \u001b[0msnapshot_step\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 339\u001b[1;33m                                                        show_metric)\n\u001b[0m\u001b[0;32m    340\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    341\u001b[0m                             \u001b[1;31m# Update training state\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tflearn\\helpers\\trainer.py\u001b[0m in \u001b[0;36m_train\u001b[1;34m(self, training_step, snapshot_epoch, snapshot_step, show_metric)\u001b[0m\n\u001b[0;32m    816\u001b[0m         \u001b[0mtflearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_training\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msession\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msession\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    817\u001b[0m         _, train_summ_str = self.session.run([self.train, self.summ_op],\n\u001b[1;32m--> 818\u001b[1;33m                                              feed_batch)\n\u001b[0m\u001b[0;32m    819\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    820\u001b[0m         \u001b[1;31m# Retrieve loss value from summary string\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    948\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    949\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 950\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    951\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    952\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1171\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1172\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[1;32m-> 1173\u001b[1;33m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[0;32m   1174\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1175\u001b[0m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1348\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1349\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[1;32m-> 1350\u001b[1;33m                            run_metadata)\n\u001b[0m\u001b[0;32m   1351\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1352\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1354\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1355\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1356\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1357\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1358\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[1;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[0;32m   1339\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1340\u001b[0m       return self._call_tf_sessionrun(\n\u001b[1;32m-> 1341\u001b[1;33m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[0;32m   1342\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1343\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[1;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[0;32m   1427\u001b[0m     return tf_session.TF_SessionRun_wrapper(\n\u001b[0;32m   1428\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1429\u001b[1;33m         run_metadata)\n\u001b[0m\u001b[0;32m   1430\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1431\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_call_tf_sessionprun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import tflearn\n",
    "from tflearn.layers.conv import conv_2d,max_pool_2d\n",
    "from tflearn.layers.core import input_data,dropout,fully_connected\n",
    "from tflearn.layers.estimator import regression\n",
    "import numpy as np\n",
    "import cv2\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "#Load Images from Swing\n",
    "loadedImages = []\n",
    "for j in range(0,32):\n",
    "    for i in range(1, 180):\n",
    "      path=\"C:/Users/NIKHIL/Desktop/Mydataset/\"+str(j)+\"/\"+str(j)+\" \"+\"(\"+str(i)+\").jpg\"\n",
    "      #print(path)\n",
    "      image = cv2.imread(path)\n",
    "      #print(image)\n",
    "      gray_image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "      loadedImages.append(gray_image.reshape(89, 100, 1))\n",
    "\n",
    "\n",
    "# Create OutputVector\n",
    "\n",
    "outputVectors = []\n",
    "for j in range(0,32):\n",
    "    listobj = [0]*32\n",
    "    listobj[j] = 1\n",
    "    for i in range(0, 179):\n",
    "      outputVectors.append(listobj)\n",
    "\n",
    "\n",
    "testImages = []\n",
    "for j in range(0,32):\n",
    "    for i in range(181, 280):\n",
    "      image = cv2.imread(\"C:/Users/NIKHIL/Desktop/Mydataset/\"+str(j)+\"/\" +str(j)+\" \"+\"(\"+str(i)+\").jpg\")\n",
    "      #print(\"test\",image)\n",
    "      gray_image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "      testImages.append(gray_image.reshape(89, 100, 1))\n",
    "\n",
    "\n",
    "testLabels = []\n",
    "for j in range(0,32):\n",
    "    listobj1 = [0]*32\n",
    "    listobj1[j] = 1 \n",
    "    for i in range(181, 280):\n",
    "      testLabels.append(listobj1)\n",
    "\n",
    "# Define the CNN Model\n",
    "tf.reset_default_graph()\n",
    "convnet=input_data(shape=[None,89,100,1],name='input')\n",
    "convnet=conv_2d(convnet,32,2,activation='relu')\n",
    "convnet=max_pool_2d(convnet,2)\n",
    "convnet=conv_2d(convnet,64,2,activation='relu')\n",
    "convnet=max_pool_2d(convnet,2)\n",
    "\n",
    "convnet=conv_2d(convnet,128,2,activation='relu')\n",
    "convnet=max_pool_2d(convnet,2)\n",
    "\n",
    "convnet=conv_2d(convnet,256,2,activation='relu')\n",
    "convnet=max_pool_2d(convnet,2)\n",
    "\n",
    "convnet=conv_2d(convnet,256,2,activation='relu')\n",
    "convnet=max_pool_2d(convnet,2)\n",
    "\n",
    "convnet=conv_2d(convnet,128,2,activation='relu')\n",
    "convnet=max_pool_2d(convnet,2)\n",
    "\n",
    "convnet=conv_2d(convnet,64,2,activation='relu')\n",
    "convnet=max_pool_2d(convnet,2)\n",
    "\n",
    "convnet=fully_connected(convnet,1000,activation='relu')\n",
    "convnet=dropout(convnet,0.75)\n",
    "\n",
    "convnet=fully_connected(convnet,32,activation='softmax')\n",
    "\n",
    "convnet=regression(convnet,optimizer='adam',learning_rate=0.001,loss='categorical_crossentropy',name='regression')\n",
    "\n",
    "model=tflearn.DNN(convnet,tensorboard_verbose=0)\n",
    "\n",
    "\n",
    "# Shuffle Training Data\n",
    "loadedImages, outputVectors = shuffle(loadedImages, outputVectors, random_state=0)\n",
    "\n",
    "# Train model\n",
    "model.fit(loadedImages, outputVectors, n_epoch=50,\n",
    "           validation_set = (testImages, testLabels),\n",
    "           snapshot_step=100, show_metric=True, run_id='convnet_coursera')\n",
    "\n",
    "model.save(\"TrainedModel/GestureRecogModel.tfl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from C:\\Users\\NIKHIL\\Desktop\\Mydataset\\TrainedModel\\GestureRecogModel.tfl\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import tflearn\n",
    "from tflearn.layers.conv import conv_2d,max_pool_2d\n",
    "from tflearn.layers.core import input_data,dropout,fully_connected\n",
    "from tflearn.layers.estimator import regression\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import cv2\n",
    "import imutils\n",
    "import time\n",
    "from gtts import gTTS \n",
    "import os\n",
    "import sys\n",
    "\n",
    "# global variables\n",
    "textdisplay = []\n",
    "bg = None\n",
    "\n",
    "def resizeImage(imageName):\n",
    "    basewidth = 100\n",
    "    img = Image.open(imageName)\n",
    "    wpercent = (basewidth/float(img.size[0]))\n",
    "    hsize = int((float(img.size[1])*float(wpercent)))\n",
    "    img = img.resize((basewidth,hsize), Image.ANTIALIAS)\n",
    "    img.save(imageName)\n",
    "\n",
    "def run_avg(image, aWeight):\n",
    "    global bg\n",
    "    # initialize the background\n",
    "    if bg is None:\n",
    "        bg = image.copy().astype(\"float\")\n",
    "        return\n",
    "\n",
    "    # compute weighted average, accumulate it and update the background\n",
    "    cv2.accumulateWeighted(image, bg, aWeight)\n",
    "\n",
    "def segment(image, threshold=25):\n",
    "    global bg\n",
    "    # find the absolute difference between background and current frame\n",
    "    diff = cv2.absdiff(bg.astype(\"uint8\"), image)\n",
    "\n",
    "    # threshold the diff image so that we get the foreground\n",
    "    thresholded = cv2.threshold(diff,\n",
    "                                threshold,\n",
    "                                255,\n",
    "                                cv2.THRESH_BINARY)[1]\n",
    "\n",
    "    # get the contours in the thresholded image\n",
    "    (cnts, _) = cv2.findContours(thresholded.copy(),\n",
    "                                    cv2.RETR_EXTERNAL,\n",
    "                                    cv2.CHAIN_APPROX_SIMPLE)\n",
    "\n",
    "    # return None, if no contours detected\n",
    "    if len(cnts) == 0:\n",
    "        return\n",
    "    else:\n",
    "        # based on contour area, get the maximum contour which is the hand\n",
    "        segmented = max(cnts, key=cv2.contourArea)\n",
    "        return (thresholded, segmented)\n",
    "\n",
    "def main():\n",
    "    # initialize weight for running average\n",
    "    aWeight = 0.5\n",
    "\n",
    "    # get the reference to the webcam\n",
    "    camera = cv2.VideoCapture(0)\n",
    "\n",
    "    # region of interest (ROI) coordinates\n",
    "    top, right, bottom, left = 10, 350, 225, 590\n",
    "\n",
    "    # initialize num of frames\n",
    "    num_frames = 0\n",
    "    start_recording = False\n",
    "\n",
    "    # keep looping, until interrupted\n",
    "    while(True):\n",
    "        # get the current frame\n",
    "        (grabbed, frame) = camera.read()\n",
    "\n",
    "        # resize the frame\n",
    "        frame = imutils.resize(frame, width = 700)\n",
    "\n",
    "        # flip the frame so that it is not the mirror view\n",
    "        frame = cv2.flip(frame, 1)\n",
    "\n",
    "        # clone the frame\n",
    "        clone = frame.copy()\n",
    "\n",
    "        # get the height and width of the frame\n",
    "        (height, width) = frame.shape[:2]\n",
    "\n",
    "        # get the ROI\n",
    "        roi = frame[top:bottom, right:left]\n",
    "\n",
    "        # convert the roi to grayscale and blur it\n",
    "        gray = cv2.cvtColor(roi, cv2.COLOR_BGR2GRAY)\n",
    "        gray = cv2.GaussianBlur(gray, (7, 7), 0)\n",
    "\n",
    "        # to get the background, keep looking till a threshold is reached\n",
    "        # so that our running average model gets calibrated\n",
    "        if num_frames < 30:\n",
    "            run_avg(gray, aWeight)\n",
    "        else:\n",
    "            # segment the hand region\n",
    "            hand = segment(gray)\n",
    "\n",
    "            # check whether hand region is segmented\n",
    "            if hand is not None:\n",
    "                # if yes, unpack the thresholded image and\n",
    "                # segmented region\n",
    "                (thresholded, segmented) = hand\n",
    "\n",
    "                # draw the segmented region and display the frame\n",
    "                cv2.drawContours(clone, [segmented + (right, top)], -1, (0, 0, 255))\n",
    "                if start_recording:\n",
    "                    cv2.imwrite('Temp.png', thresholded)\n",
    "                    resizeImage('Temp.png')\n",
    "                    predictedClass, confidence = getPredictedClass()\n",
    "                    showStatistics(predictedClass, confidence)\n",
    "                cv2.imshow(\"Thesholded\", thresholded)\n",
    "\n",
    "        # draw the segmented hand\n",
    "        cv2.rectangle(clone, (left, top), (right, bottom), (0,255,0), 2)\n",
    "\n",
    "        # increment the number of frames\n",
    "        num_frames += 1\n",
    "\n",
    "        # display the frame with segmented hand\n",
    "        cv2.imshow(\"Video Feed\", clone)\n",
    "\n",
    "        # observe the keypress by the user\n",
    "        keypress = cv2.waitKey(1) & 0xFF\n",
    "\n",
    "        # if the user pressed \"q\", then stop looping\n",
    "        if keypress == ord(\"q\"):\n",
    "#             sys.exit()\n",
    "            break\n",
    "        \n",
    "        if keypress == ord(\"s\"):\n",
    "            start_recording = True\n",
    "    camera.release()\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "def getPredictedClass():\n",
    "    # Predict\n",
    "    image = cv2.imread('Temp.png')\n",
    "    gray_image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    prediction = model.predict([gray_image.reshape(89, 100, 1)])\n",
    "    sum1 = 0\n",
    "    for i in range(0,32):\n",
    "        sum1 += prediction[0][i]  \n",
    "    return np.argmax(prediction), (np.amax(prediction) / (sum1))\n",
    "\n",
    "def showStatistics(predictedClass, confidence):\n",
    "\n",
    "    textImage = np.zeros((300,512,3), np.uint8)\n",
    "    className = \"\"\n",
    "    if predictedClass == 0:\n",
    "        className = \"A\"\n",
    "    elif predictedClass == 1:\n",
    "        className = \"B\"\n",
    "    elif predictedClass == 2:\n",
    "        className = \"C\"\n",
    "    elif predictedClass == 3:\n",
    "        className = \"D\"\n",
    "    elif predictedClass == 4:\n",
    "        className = \"E\"\n",
    "    elif predictedClass == 5:\n",
    "        className = \"F\"\n",
    "    elif predictedClass == 6:\n",
    "        className = \"G\"\n",
    "    elif predictedClass == 7:\n",
    "        className = \"H\"\n",
    "    elif predictedClass == 8:\n",
    "        className = \"I\"\n",
    "    elif predictedClass == 9:\n",
    "        className = \"J\"\n",
    "    elif predictedClass == 10:\n",
    "        className = \"K\"\n",
    "    elif predictedClass == 11:\n",
    "        className = \"L\"\n",
    "    elif predictedClass == 12:\n",
    "        className = \"M\"\n",
    "    elif predictedClass == 13:\n",
    "        className = \"N\"\n",
    "    elif predictedClass == 14:\n",
    "        className = \"O\"\n",
    "    elif predictedClass == 15:\n",
    "        className = \"P\"\n",
    "    elif predictedClass == 16:\n",
    "        className = \"Q\"\n",
    "    elif predictedClass == 17:\n",
    "        className = \"R\"\n",
    "    elif predictedClass == 18:\n",
    "        className = \"S\"\n",
    "    elif predictedClass == 19:\n",
    "        className = \"T\"\n",
    "    elif predictedClass == 20:\n",
    "        className = \"U\"\n",
    "    elif predictedClass == 21:\n",
    "        className = \"V\"\n",
    "    elif predictedClass == 22:\n",
    "        className = \"W\"\n",
    "    elif predictedClass == 23:\n",
    "        className = \"X\"\n",
    "    elif predictedClass == 24:\n",
    "        className = \"Y\"\n",
    "    elif predictedClass == 25:\n",
    "        className = \"Z\"\n",
    "    elif predictedClass == 26:\n",
    "        className = \"All the Best\"\n",
    "    elif predictedClass == 27:\n",
    "        className = \"Palm\"\n",
    "    elif predictedClass == 29:\n",
    "        className = \"Yolo\"\n",
    "    elif predictedClass == 30:\n",
    "        className = \" \"\n",
    "    elif predictedClass == 31:\n",
    "        className = \"Delete\"\n",
    "    elif predictedClass == 28:\n",
    "        className = \"Shift\"\n",
    "        \n",
    "    \n",
    "    \n",
    "    \n",
    "    keypress = cv2.waitKey(1) & 0xFF\n",
    "    if keypress == ord(\"y\"):\n",
    "        if className == \"Delete\":\n",
    "            textdisplay.pop()\n",
    "        else:\n",
    "            textdisplay.append(className)\n",
    "    str3 = \" \"\n",
    "    for i in textdisplay:\n",
    "        str3 += i\n",
    "    cv2.putText(textImage,\"Text: \" + str3, \n",
    "    (30, 200), \n",
    "    cv2.FONT_HERSHEY_SIMPLEX, \n",
    "    1,\n",
    "    (255, 255, 255),\n",
    "    2)\n",
    "        \n",
    "    cv2.putText(textImage,\"Predicted Class : \" + className, \n",
    "    (30, 30), \n",
    "    cv2.FONT_HERSHEY_SIMPLEX, \n",
    "    1,\n",
    "    (255, 255, 255),\n",
    "    2)\n",
    "\n",
    "    cv2.putText(textImage,\"Confidence : \" + str(confidence * 100) + '%', \n",
    "    (30, 100),\n",
    "    cv2.FONT_HERSHEY_SIMPLEX, \n",
    "    1,\n",
    "    (255, 255, 255),\n",
    "    2)\n",
    "    cv2.imshow(\"Statistics\", textImage)\n",
    "    keypress1 = cv2.waitKey(1) & 0xFF\n",
    "    if keypress1 == ord(\"v\"):\n",
    "        mytext = str3\n",
    "        language = 'en'\n",
    "        myobj = gTTS(text=mytext, lang=language, slow=False)\n",
    "        myobj.save(\"voice.mp3\")\n",
    "        os.system(\"start voice.mp3\")\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "# Model defined\n",
    "tf.reset_default_graph()\n",
    "convnet=input_data(shape=[None,89,100,1],name='input')\n",
    "convnet=conv_2d(convnet,32,2,activation='relu')\n",
    "convnet=max_pool_2d(convnet,2)\n",
    "convnet=conv_2d(convnet,64,2,activation='relu')\n",
    "convnet=max_pool_2d(convnet,2)\n",
    "\n",
    "convnet=conv_2d(convnet,128,2,activation='relu')\n",
    "convnet=max_pool_2d(convnet,2)\n",
    "\n",
    "convnet=conv_2d(convnet,256,2,activation='relu')\n",
    "convnet=max_pool_2d(convnet,2)\n",
    "\n",
    "convnet=conv_2d(convnet,256,2,activation='relu')\n",
    "convnet=max_pool_2d(convnet,2)\n",
    "\n",
    "convnet=conv_2d(convnet,128,2,activation='relu')\n",
    "convnet=max_pool_2d(convnet,2)\n",
    "\n",
    "convnet=conv_2d(convnet,64,2,activation='relu')\n",
    "convnet=max_pool_2d(convnet,2)\n",
    "\n",
    "convnet=fully_connected(convnet,1000,activation='relu')\n",
    "convnet=dropout(convnet,0.75)\n",
    "\n",
    "convnet=fully_connected(convnet,32,activation='softmax')\n",
    "\n",
    "convnet=regression(convnet,optimizer='adam',learning_rate=0.001,loss='categorical_crossentropy',name='regression')\n",
    "\n",
    "model=tflearn.DNN(convnet,tensorboard_verbose=0)\n",
    "\n",
    "# Load Saved Model\n",
    "model.load(\"TrainedModel/GestureRecogModel.tfl\")\n",
    "\n",
    "main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
